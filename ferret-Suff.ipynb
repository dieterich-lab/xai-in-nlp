{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a236abef-0f4e-483b-8fc5-c8eda47986ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ferret import Benchmark\n",
    "from statistics import mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3806a3-b861-4391-8d21-411a3bbe07bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4536ba06-8bed-4ae4-80ef-aaec59f3c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aaa62c9a-5903-49a1-a1e4-6ed674a4d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"/beegfs/scratch/rsari/BertSeqCA/checkpoint-10000/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b10116-de23-4fb6-98bb-3e078a38dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anrede Diagnosen AllergienUnverträglichkeitenRisiken Anamnese Medikation KUBefunde Befunde EchoBefunde Zusammenfassung Mix Abschluss\n"
     ]
    }
   ],
   "source": [
    "id2tag = {0: 'Anrede', 1: 'Diagnosen', 2: 'AllergienUnverträglichkeitenRisiken', 3: 'Anamnese', 4: 'Medikation', 5: 'KUBefunde', 6: 'Befunde', 7: 'EchoBefunde', 8: 'Zusammenfassung', 9: 'Mix', 10: 'Abschluss'}\n",
    "tag2id = {tag: id for id, tag in id2tag.items()}\n",
    "\n",
    "labels = list(id2tag.values())\n",
    "print(\" \".join(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda40532-e9c5-45da-a68b-719c4e0a4484",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which of the interpretability methods is more faithful ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02a2cc-9f57-44a0-b9ba-c9b6a94ed3bd",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. Calculate for IG & SHAP each Comprehensiveness & Sufficiency\n",
    "    1. Which of them performs better in each ?\n",
    "        1. Reasons?\n",
    "    2. Which labels are conspicious for scoring high/low or \n",
    "        1. Metric specific tendencies ?\n",
    "        2. Data Bias ?\n",
    "    3. Finally IG or SHAP \"better\" overall ?\n",
    "        1. Pros/Cons of approach with Faithfulness metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ef02a0e4-0eb9-4448-943d-bb763edd8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = Benchmark(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c243d98-cb40-4d8f-a55c-29b278e3da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anrede', 11),\n",
       " ('Diagnosen', 10),\n",
       " ('AllergienUnverträglichkeitenRisiken', 13),\n",
       " ('Anamnese', 14),\n",
       " ('Medikation', 10),\n",
       " ('KUBefunde', 11),\n",
       " ('Befunde', 12),\n",
       " ('EchoBefunde', 17),\n",
       " ('Zusammenfassung', 13),\n",
       " ('Mix', 21),\n",
       " ('Abschluss', 16)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "[(k, len(v)) for k,v in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cf5343-bf9e-4633-aa50-2214a66067a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ig.p\", \"rb\") as f:\n",
    "    ig = pickle.load(f)\n",
    "    \n",
    "with open(\"shap.p\", \"rb\") as f:\n",
    "    shap = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_ig.p\", \"rb\") as f:\n",
    "    eva_ig = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_shap.p\", \"rb\") as f:\n",
    "    eva_shap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf6243f-5f00-4101-92ef-f719e8d1707a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anrede [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10]\n",
      "Diagnosen [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "AllergienUnverträglichkeitenRisiken [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12]\n",
      "Anamnese [0, 1, 2, 3, 4, 6, 9, 10, 11, 12] [5, 7, 8, 13]\n",
      "Medikation [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "KUBefunde [0, 1, 2, 3, 5, 6, 7, 8, 9, 10] [4]\n",
      "Befunde [0, 1, 2, 3, 4, 5, 6, 8, 9, 10] [7, 11]\n",
      "EchoBefunde [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12, 13, 14, 15, 16]\n",
      "Zusammenfassung [0, 1, 2, 3, 4, 6, 7, 8, 9, 10] [5, 11, 12]\n",
      "Mix [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Abschluss [0, 2, 3, 4, 5, 6, 7, 8, 9, 11] [1, 10, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "compr, suff = {l: None for l in [\"Anrede\"]}, {l: None for l in [\"Anrede\"]}\n",
    "\n",
    "for l in labels:\n",
    "    right = [data[l].index(d) for d in data[l] if d[0] == l]\n",
    "    wrong = [data[l].index(d) for d in data[l] if d[0] != l]\n",
    "    print(l, right, wrong)\n",
    "    compr_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    compr_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    \n",
    "    suff_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    suff_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    \n",
    "    compr[l] = {\"IG\": (np.nanmean([e for e in compr_ig if compr_ig.index(e) in right]), np.nanmean([e for e in compr_ig if compr_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in compr_shap if compr_shap.index(e) in right]), np.nanmean([e for e in compr_shap if compr_shap.index(e) in wrong]))}\n",
    "    suff[l] = {\"IG\": (np.nanmean([e for e in suff_ig if suff_ig.index(e) in right]), np.nanmean([e for e in suff_ig if suff_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in suff_shap if suff_shap.index(e) in right]), np.nanmean([e for e in suff_shap if suff_shap.index(e) in wrong]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b16b07-ef35-40b7-a4c4-f7b8b1d77cf4",
   "metadata": {},
   "source": [
    "## **2. Sufficiency**\n",
    "Measures if top k% (10 step: 10-100) tokens in explanation are sufficient for the right prediction: $f(x)_j - f(r_j)_j$  \n",
    "**Lower score indicates that exclusion of tokens in $r_j$ highlighted by explainer are actually important for prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7a3eb-930f-4349-96b6-c1dd84d5bd8e",
   "metadata": {},
   "source": [
    "### 2.1. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a9df8f-9a65-4fcb-b79a-1aef1dd4bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "Zusammenfassung                      [[0.0, 0.01]]\n",
      "Anamnese                             [[0.02, 0.05]]\n",
      "Befunde                              [[0.02, 0.02]]\n",
      "EchoBefunde                          [[0.04, nan]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.06, nan]]\n",
      "Medikation                           [[0.07, nan]]\n",
      "Anrede                               [[0.1, nan]]\n",
      "Abschluss                            [[0.13, 0.35]]\n",
      "Diagnosen                            [[0.19, nan]]\n",
      "KUBefunde                            [[0.26, 0.12]]\n",
      "Mix                                  [[0.4, 0.1]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k == \"SHAP\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1edf7238-b568-4af9-93dd-3125c728ce45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " [(28,\n",
       "   'Die cMRT am Folgetag zeigte mehrere kleine Infarkte im Hirnstamm sowie rechts in den Stammganglien und bds.',\n",
       "   Evaluation(name='aopc_suff', score=-0.002510023)),\n",
       "  (17,\n",
       "   'Röntgenologisch wurde der V.a. eine Stauungspneumonie gestellt.',\n",
       "   Evaluation(name='aopc_suff', score=9.805957e-05)),\n",
       "  (85,\n",
       "   'Es erging die Maßgabe das Konzept vom letzten stationären Aufenthalt fortzuführen -LRB- Vorstellung in der Thoraxklinik -RRB- und die damals begonnene Therapie mit Methylprednisolon wöchentlich um 2,5 mg weiter auszuschleichen -LRB- letzte Reduktion am <[Pseudo] 14/10/2033> auf 7,5 mg -RRB- .',\n",
       "   Evaluation(name='aopc_suff', score=0.00029241375)),\n",
       "  (34,\n",
       "   'Zur Vorbereitung auf die MitraClip erfolgte am <[Pseudo] 21/06/2034> eine Farbduplexsonographie der Karotiden.',\n",
       "   Evaluation(name='aopc_suff', score=0.005224265)),\n",
       "  (41,\n",
       "   'Wir bitten um eine Trinkmengenrestriktion von 1,5 l/d und regelmäßige laborchemische Kontrollen der Elektrolyte durch den Hausarzt mit ggf. Einleitung einer weitergehenden Abklärung.',\n",
       "   Evaluation(name='aopc_suff', score=0.00016551359)),\n",
       "  (8,\n",
       "   'Der Patient wurde hierüber umfassend aufgeklärt.',\n",
       "   Evaluation(name='aopc_suff', score=0.010112286)),\n",
       "  (58,\n",
       "   'Echokardiographisch zeigte sich ein dilatierter linker Ventrikel mit hochgradig verminderter systolischer Pumpfunktion bei Akinesie des gesamten Apex, inferior, inferolateral sowie Hypokinesie der übrigen Wandabschnitte.',\n",
       "   Evaluation(name='aopc_suff', score=0.00039588413)),\n",
       "  (29,\n",
       "   'In Zusammenschau der Befunde wurde der Verdacht auf eine spontane Rekanalisation oder ein Artefakt in der CT-Untersuchung gestellt.',\n",
       "   Evaluation(name='aopc_suff', score=0.00023282568)),\n",
       "  (9,\n",
       "   'Für Rückfragen stehen wir jederzeit zur Verfügung.',\n",
       "   Evaluation(name='aopc_suff', score=0.011528194)),\n",
       "  (30,\n",
       "   'In diesem Zusammenhang wurde auch erneut auf zusätzliche Kalorienzufuhr im Rahme der Peritonealdialyse durch die Glucoselösung hingewiesen.',\n",
       "   Evaluation(name='aopc_suff', score=-0.00011472191))])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eva_shap[\"Zusammenfassung\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_shap[\"Zusammenfassung\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9978b-2448-4987-a72a-89e8b683fe17",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "726f325e-d511-43cb-b60c-e702f3c18e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Röntgenologisch wurde der V.a. eine Stauungspneumonie gestellt. \tScore: 1.0\n",
      "Filtered: [('[MASK]', -0.1063335279302019), ('##ologisch', 0.057876511226732985), ('wurde', 0.44221082629923775), ('der', 0.10508947177137497), ('[MASK]', -0.018277182651793344), ('[MASK]', -0.003599826201779736), ('[MASK]', -0.026887146833413894), ('[MASK]', -0.012654872553857098), ('eine', 0.08299531199478645), ('[MASK]', -0.008186067217991968), ('[MASK]', -0.009768548961348754), ('##p', 0.021391422173720247), ('##ne', 0.0075568083103108846), ('[MASK]', -0.02342996950130498), ('##onie', 0.0028170837737001328), ('gestellt', 0.057691346262249435), ('.', 0.013234024963402305)]\n",
      "\n",
      "1 important token(s) only: 'wurde' affects original score: 0.0 | Labeled: Zusammenfassung: 0.994884192943573\n",
      "2 important token(s) only: 'wurde der' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9962839484214783\n",
      "3 important token(s) only: 'wurde der eine' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9972719550132751\n",
      "4 important token(s) only: '##ologisch wurde der eine' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9972208738327026\n",
      "4 important token(s) only: '##ologisch wurde der eine' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9972208738327026\n",
      "5 important token(s) only: '##ologisch wurde der eine gestellt' affects original score: 0.0 | Labeled: Zusammenfassung: 0.9948465824127197\n",
      "6 important token(s) only: '##ologisch wurde der einep gestellt' affects original score: 0.0 | Labeled: Zusammenfassung: 0.9938479065895081\n",
      "7 important token(s) only: '##ologisch wurde der einep gestellt.' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9963029623031616\n",
      "8 important token(s) only: '##ologisch wurde der einepne gestellt.' affects original score: 0.0 | Labeled: Zusammenfassung: 0.9949859380722046\n",
      "9 important token(s) only: '##ologisch wurde der einepneonie gestellt.' affects original score: -0.0 | Labeled: Zusammenfassung: 0.995965838432312\n",
      "\n",
      "Mean of all scores: -0.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874ddaa-dbc1-45e2-bac4-4ef29032ea8a",
   "metadata": {},
   "source": [
    "### Ablation Study: Inclusion of neg. attr. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2e354500-6304-4275-a03d-b14d63ccbfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Röntgenologisch wurde der V.a. eine Stauungspneumonie gestellt. \tScore: 1.0\n",
      "Filtered: [('Röntgen', -0.1063335279302019), ('##ologisch', 0.057876511226732985), ('wurde', 0.44221082629923775), ('der', 0.10508947177137497), ('V', -0.018277182651793344), ('.', -0.003599826201779736), ('a', -0.026887146833413894), ('.', -0.012654872553857098), ('eine', 0.08299531199478645), ('Stau', -0.008186067217991968), ('##ungs', -0.009768548961348754), ('##p', 0.021391422173720247), ('##ne', 0.0075568083103108846), ('##um', -0.02342996950130498), ('##onie', 0.0028170837737001328), ('gestellt', 0.057691346262249435), ('.', 0.013234024963402305)]\n",
      "\n",
      "2 important token(s) only: 'Röntgen wurde' affects original score: 0.17 | Labeled: Zusammenfassung: 0.829968273639679\n",
      "3 important token(s) only: 'Röntgen wurde der' affects original score: 0.02 | Labeled: Zusammenfassung: 0.9789688587188721\n",
      "5 important token(s) only: 'Röntgenologisch wurde der eine' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9977218508720398\n",
      "7 important token(s) only: 'Röntgenologisch wurde der a eine gestellt' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9965625405311584\n",
      "8 important token(s) only: 'Röntgenologisch wurde der a eineum gestellt' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9963600039482117\n",
      "10 important token(s) only: 'Röntgenologisch wurde der V a einepum gestellt' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9957253932952881\n",
      "12 important token(s) only: 'Röntgenologisch wurde der V a. einepum gestellt.' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9959049820899963\n",
      "14 important token(s) only: 'Röntgenologisch wurde der V a. eine Stauungspum gestellt.' affects original score: -0.0 | Labeled: Zusammenfassung: 0.995919406414032\n",
      "15 important token(s) only: 'Röntgenologisch wurde der V a. eine Stauungspneum gestellt.' affects original score: -0.0 | Labeled: Zusammenfassung: 0.9954180717468262\n",
      "17 important token(s) only: 'Röntgenologisch wurde der V. a. eine Stauungspneumonie gestellt.' affects original score: 0.0 | Labeled: Zusammenfassung: 0.9953984618186951\n",
      "\n",
      "Mean of all scores: 0.02\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e7755-35ca-4025-8eb6-31c28ac22df9",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens no significant difference** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8e19545b-ffe1-42d9-91ab-8a8115a700b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Prediction swayed by removal of negative contributing tokens to label:  Befunde\n",
      "Original sentence: Röntgenologisch wurde der V.a. eine Stauungspneumonie gestellt. \tScore: 1.0\n",
      "Filtered: [('Röntgen', 0.13422073809435256), ('[MASK]', -0.0792783709766035), ('[MASK]', -0.3623967811402128), ('[MASK]', -0.037282700167099045), ('[MASK]', -0.059239771203730175), ('.', 0.01484986202020697), ('a', 0.03773797687690534), ('[MASK]', -0.006395933277709508), ('[MASK]', -0.018892764247409405), ('Stau', 0.031500919813153686), ('[MASK]', -0.01029594943450585), ('##p', 0.008822604181263963), ('##ne', 0.00554474005959565), ('[MASK]', -0.010698659574987624), ('##onie', 0.04106052900256171), ('[MASK]', -0.10094436338971795), ('[MASK]', -0.005396102130368896)]\n",
      "\n",
      "1 important token(s) only: 'Röntgen' affects original score: 0.99 | Labeled: Befunde: 0.9646967053413391\n",
      "1 important token(s) only: 'Röntgen' affects original score: 0.99 | Labeled: Befunde: 0.9646967053413391\n",
      "2 important token(s) only: 'Röntgenonie' affects original score: 0.99 | Labeled: Befunde: 0.9698911309242249\n",
      "3 important token(s) only: 'Röntgen aonie' affects original score: 0.99 | Labeled: Befunde: 0.9732292890548706\n",
      "4 important token(s) only: 'Röntgen a Stauonie' affects original score: 0.99 | Labeled: Befunde: 0.9890201687812805\n",
      "4 important token(s) only: 'Röntgen a Stauonie' affects original score: 0.99 | Labeled: Befunde: 0.9890201687812805\n",
      "5 important token(s) only: 'Röntgen. a Stauonie' affects original score: 0.99 | Labeled: Befunde: 0.992622971534729\n",
      "6 important token(s) only: 'Röntgen. a Stauponie' affects original score: 0.99 | Labeled: Befunde: 0.9926825165748596\n",
      "6 important token(s) only: 'Röntgen. a Stauponie' affects original score: 0.99 | Labeled: Befunde: 0.9926825165748596\n",
      "7 important token(s) only: 'Röntgen. a Staupneonie' affects original score: 0.99 | Labeled: Befunde: 0.9924853444099426\n",
      "\n",
      "Mean of all scores: 0.99\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target, normalize_scores=True)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e30f465a-eead-4d0e-b0b0-9f43aedc6147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f3051_row0_col0 {\n",
       "  background-color: #dee5eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col1, #T_f3051_row0_col15, #T_f3051_row1_col12, #T_f3051_row2_col8, #T_f3051_row3_col16 {\n",
       "  background-color: #f1e7e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col2 {\n",
       "  background-color: #e8a1a6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col3, #T_f3051_row5_col15 {\n",
       "  background-color: #f0dedf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col4, #T_f3051_row0_col13, #T_f3051_row1_col15, #T_f3051_row4_col8 {\n",
       "  background-color: #edeff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col5, #T_f3051_row4_col7, #T_f3051_row4_col16, #T_f3051_row5_col6 {\n",
       "  background-color: #f1f1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col6, #T_f3051_row1_col7, #T_f3051_row5_col0 {\n",
       "  background-color: #eceef1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col7, #T_f3051_row0_col9, #T_f3051_row0_col10, #T_f3051_row3_col13, #T_f3051_row4_col10, #T_f3051_row4_col13, #T_f3051_row5_col4, #T_f3051_row5_col7 {\n",
       "  background-color: #eff0f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col8, #T_f3051_row1_col3, #T_f3051_row1_col8 {\n",
       "  background-color: #f0e3e3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col11, #T_f3051_row1_col6, #T_f3051_row1_col9, #T_f3051_row1_col13, #T_f3051_row2_col11, #T_f3051_row2_col13, #T_f3051_row3_col3, #T_f3051_row3_col5 {\n",
       "  background-color: #f2eeee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col12, #T_f3051_row0_col14, #T_f3051_row1_col11, #T_f3051_row4_col12, #T_f3051_row5_col5 {\n",
       "  background-color: #f2f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row0_col16, #T_f3051_row1_col14, #T_f3051_row3_col9, #T_f3051_row3_col10, #T_f3051_row4_col5, #T_f3051_row4_col11 {\n",
       "  background-color: #f2efef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row1_col0, #T_f3051_row4_col1 {\n",
       "  background-color: #e2e8ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row1_col1, #T_f3051_row1_col5, #T_f3051_row2_col4, #T_f3051_row3_col1, #T_f3051_row5_col16 {\n",
       "  background-color: #f1e8e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row1_col2 {\n",
       "  background-color: #e9aeb2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row1_col4, #T_f3051_row1_col16, #T_f3051_row4_col3 {\n",
       "  background-color: #ebeef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row1_col10, #T_f3051_row2_col10, #T_f3051_row2_col12, #T_f3051_row3_col11, #T_f3051_row3_col14, #T_f3051_row5_col9, #T_f3051_row5_col10, #T_f3051_row5_col14 {\n",
       "  background-color: #f2eded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col0 {\n",
       "  background-color: #efd4d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col1 {\n",
       "  background-color: #f0ddde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col2, #T_f3051_row3_col6, #T_f3051_row3_col15 {\n",
       "  background-color: #f0e1e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col3, #T_f3051_row2_col9, #T_f3051_row3_col4, #T_f3051_row4_col14, #T_f3051_row5_col1 {\n",
       "  background-color: #f1eaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col5, #T_f3051_row2_col7, #T_f3051_row2_col14, #T_f3051_row2_col16, #T_f3051_row3_col12, #T_f3051_row4_col6, #T_f3051_row4_col9, #T_f3051_row5_col11, #T_f3051_row5_col12, #T_f3051_row5_col13 {\n",
       "  background-color: #f2ebeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col6, #T_f3051_row5_col3 {\n",
       "  background-color: #f1e5e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row2_col15 {\n",
       "  background-color: #f1e4e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row3_col0, #T_f3051_row3_col8 {\n",
       "  background-color: #efdbdd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row3_col2 {\n",
       "  background-color: #dae3e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row3_col7 {\n",
       "  background-color: #e8ecef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row4_col0 {\n",
       "  background-color: #efd9da;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row4_col2 {\n",
       "  background-color: #b1c7d7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row4_col4 {\n",
       "  background-color: #e7ebee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row4_col15 {\n",
       "  background-color: #e0e6eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row5_col2 {\n",
       "  background-color: #ebb9bd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f3051_row5_col8 {\n",
       "  background-color: #eed0d2;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f3051\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f3051_level0_col0\" class=\"col_heading level0 col0\" >Röntgen</th>\n",
       "      <th id=\"T_f3051_level0_col1\" class=\"col_heading level0 col1\" >##ologisch</th>\n",
       "      <th id=\"T_f3051_level0_col2\" class=\"col_heading level0 col2\" >wurde</th>\n",
       "      <th id=\"T_f3051_level0_col3\" class=\"col_heading level0 col3\" >der</th>\n",
       "      <th id=\"T_f3051_level0_col4\" class=\"col_heading level0 col4\" >V</th>\n",
       "      <th id=\"T_f3051_level0_col5\" class=\"col_heading level0 col5\" >.</th>\n",
       "      <th id=\"T_f3051_level0_col6\" class=\"col_heading level0 col6\" >a</th>\n",
       "      <th id=\"T_f3051_level0_col7\" class=\"col_heading level0 col7\" >..1</th>\n",
       "      <th id=\"T_f3051_level0_col8\" class=\"col_heading level0 col8\" >eine</th>\n",
       "      <th id=\"T_f3051_level0_col9\" class=\"col_heading level0 col9\" >Stau</th>\n",
       "      <th id=\"T_f3051_level0_col10\" class=\"col_heading level0 col10\" >##ungs</th>\n",
       "      <th id=\"T_f3051_level0_col11\" class=\"col_heading level0 col11\" >##p</th>\n",
       "      <th id=\"T_f3051_level0_col12\" class=\"col_heading level0 col12\" >##ne</th>\n",
       "      <th id=\"T_f3051_level0_col13\" class=\"col_heading level0 col13\" >##um</th>\n",
       "      <th id=\"T_f3051_level0_col14\" class=\"col_heading level0 col14\" >##onie</th>\n",
       "      <th id=\"T_f3051_level0_col15\" class=\"col_heading level0 col15\" >gestellt</th>\n",
       "      <th id=\"T_f3051_level0_col16\" class=\"col_heading level0 col16\" >..2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row0\" class=\"row_heading level0 row0\" >Partition SHAP</th>\n",
       "      <td id=\"T_f3051_row0_col0\" class=\"data row0 col0\" >-0.11</td>\n",
       "      <td id=\"T_f3051_row0_col1\" class=\"data row0 col1\" >0.06</td>\n",
       "      <td id=\"T_f3051_row0_col2\" class=\"data row0 col2\" >0.44</td>\n",
       "      <td id=\"T_f3051_row0_col3\" class=\"data row0 col3\" >0.11</td>\n",
       "      <td id=\"T_f3051_row0_col4\" class=\"data row0 col4\" >-0.02</td>\n",
       "      <td id=\"T_f3051_row0_col5\" class=\"data row0 col5\" >-0.00</td>\n",
       "      <td id=\"T_f3051_row0_col6\" class=\"data row0 col6\" >-0.03</td>\n",
       "      <td id=\"T_f3051_row0_col7\" class=\"data row0 col7\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row0_col8\" class=\"data row0 col8\" >0.08</td>\n",
       "      <td id=\"T_f3051_row0_col9\" class=\"data row0 col9\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row0_col10\" class=\"data row0 col10\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row0_col11\" class=\"data row0 col11\" >0.02</td>\n",
       "      <td id=\"T_f3051_row0_col12\" class=\"data row0 col12\" >0.01</td>\n",
       "      <td id=\"T_f3051_row0_col13\" class=\"data row0 col13\" >-0.02</td>\n",
       "      <td id=\"T_f3051_row0_col14\" class=\"data row0 col14\" >0.00</td>\n",
       "      <td id=\"T_f3051_row0_col15\" class=\"data row0 col15\" >0.06</td>\n",
       "      <td id=\"T_f3051_row0_col16\" class=\"data row0 col16\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row1\" class=\"row_heading level0 row1\" >LIME</th>\n",
       "      <td id=\"T_f3051_row1_col0\" class=\"data row1 col0\" >-0.08</td>\n",
       "      <td id=\"T_f3051_row1_col1\" class=\"data row1 col1\" >0.05</td>\n",
       "      <td id=\"T_f3051_row1_col2\" class=\"data row1 col2\" >0.37</td>\n",
       "      <td id=\"T_f3051_row1_col3\" class=\"data row1 col3\" >0.08</td>\n",
       "      <td id=\"T_f3051_row1_col4\" class=\"data row1 col4\" >-0.03</td>\n",
       "      <td id=\"T_f3051_row1_col5\" class=\"data row1 col5\" >0.05</td>\n",
       "      <td id=\"T_f3051_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_f3051_row1_col7\" class=\"data row1 col7\" >-0.03</td>\n",
       "      <td id=\"T_f3051_row1_col8\" class=\"data row1 col8\" >0.08</td>\n",
       "      <td id=\"T_f3051_row1_col9\" class=\"data row1 col9\" >0.02</td>\n",
       "      <td id=\"T_f3051_row1_col10\" class=\"data row1 col10\" >0.03</td>\n",
       "      <td id=\"T_f3051_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_f3051_row1_col12\" class=\"data row1 col12\" >0.06</td>\n",
       "      <td id=\"T_f3051_row1_col13\" class=\"data row1 col13\" >0.02</td>\n",
       "      <td id=\"T_f3051_row1_col14\" class=\"data row1 col14\" >0.01</td>\n",
       "      <td id=\"T_f3051_row1_col15\" class=\"data row1 col15\" >-0.02</td>\n",
       "      <td id=\"T_f3051_row1_col16\" class=\"data row1 col16\" >-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row2\" class=\"row_heading level0 row2\" >Gradient</th>\n",
       "      <td id=\"T_f3051_row2_col0\" class=\"data row2 col0\" >0.16</td>\n",
       "      <td id=\"T_f3051_row2_col1\" class=\"data row2 col1\" >0.11</td>\n",
       "      <td id=\"T_f3051_row2_col2\" class=\"data row2 col2\" >0.09</td>\n",
       "      <td id=\"T_f3051_row2_col3\" class=\"data row2 col3\" >0.04</td>\n",
       "      <td id=\"T_f3051_row2_col4\" class=\"data row2 col4\" >0.05</td>\n",
       "      <td id=\"T_f3051_row2_col5\" class=\"data row2 col5\" >0.03</td>\n",
       "      <td id=\"T_f3051_row2_col6\" class=\"data row2 col6\" >0.06</td>\n",
       "      <td id=\"T_f3051_row2_col7\" class=\"data row2 col7\" >0.03</td>\n",
       "      <td id=\"T_f3051_row2_col8\" class=\"data row2 col8\" >0.06</td>\n",
       "      <td id=\"T_f3051_row2_col9\" class=\"data row2 col9\" >0.04</td>\n",
       "      <td id=\"T_f3051_row2_col10\" class=\"data row2 col10\" >0.02</td>\n",
       "      <td id=\"T_f3051_row2_col11\" class=\"data row2 col11\" >0.02</td>\n",
       "      <td id=\"T_f3051_row2_col12\" class=\"data row2 col12\" >0.02</td>\n",
       "      <td id=\"T_f3051_row2_col13\" class=\"data row2 col13\" >0.02</td>\n",
       "      <td id=\"T_f3051_row2_col14\" class=\"data row2 col14\" >0.04</td>\n",
       "      <td id=\"T_f3051_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_f3051_row2_col16\" class=\"data row2 col16\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row3\" class=\"row_heading level0 row3\" >Gradient (x Input)</th>\n",
       "      <td id=\"T_f3051_row3_col0\" class=\"data row3 col0\" >0.12</td>\n",
       "      <td id=\"T_f3051_row3_col1\" class=\"data row3 col1\" >0.05</td>\n",
       "      <td id=\"T_f3051_row3_col2\" class=\"data row3 col2\" >-0.13</td>\n",
       "      <td id=\"T_f3051_row3_col3\" class=\"data row3 col3\" >0.02</td>\n",
       "      <td id=\"T_f3051_row3_col4\" class=\"data row3 col4\" >0.05</td>\n",
       "      <td id=\"T_f3051_row3_col5\" class=\"data row3 col5\" >0.02</td>\n",
       "      <td id=\"T_f3051_row3_col6\" class=\"data row3 col6\" >0.09</td>\n",
       "      <td id=\"T_f3051_row3_col7\" class=\"data row3 col7\" >-0.05</td>\n",
       "      <td id=\"T_f3051_row3_col8\" class=\"data row3 col8\" >0.12</td>\n",
       "      <td id=\"T_f3051_row3_col9\" class=\"data row3 col9\" >0.01</td>\n",
       "      <td id=\"T_f3051_row3_col10\" class=\"data row3 col10\" >0.01</td>\n",
       "      <td id=\"T_f3051_row3_col11\" class=\"data row3 col11\" >0.03</td>\n",
       "      <td id=\"T_f3051_row3_col12\" class=\"data row3 col12\" >0.04</td>\n",
       "      <td id=\"T_f3051_row3_col13\" class=\"data row3 col13\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row3_col14\" class=\"data row3 col14\" >0.03</td>\n",
       "      <td id=\"T_f3051_row3_col15\" class=\"data row3 col15\" >0.09</td>\n",
       "      <td id=\"T_f3051_row3_col16\" class=\"data row3 col16\" >0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row4\" class=\"row_heading level0 row4\" >Integrated Gradient</th>\n",
       "      <td id=\"T_f3051_row4_col0\" class=\"data row4 col0\" >0.13</td>\n",
       "      <td id=\"T_f3051_row4_col1\" class=\"data row4 col1\" >-0.08</td>\n",
       "      <td id=\"T_f3051_row4_col2\" class=\"data row4 col2\" >-0.36</td>\n",
       "      <td id=\"T_f3051_row4_col3\" class=\"data row4 col3\" >-0.04</td>\n",
       "      <td id=\"T_f3051_row4_col4\" class=\"data row4 col4\" >-0.06</td>\n",
       "      <td id=\"T_f3051_row4_col5\" class=\"data row4 col5\" >0.01</td>\n",
       "      <td id=\"T_f3051_row4_col6\" class=\"data row4 col6\" >0.04</td>\n",
       "      <td id=\"T_f3051_row4_col7\" class=\"data row4 col7\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row4_col8\" class=\"data row4 col8\" >-0.02</td>\n",
       "      <td id=\"T_f3051_row4_col9\" class=\"data row4 col9\" >0.03</td>\n",
       "      <td id=\"T_f3051_row4_col10\" class=\"data row4 col10\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row4_col11\" class=\"data row4 col11\" >0.01</td>\n",
       "      <td id=\"T_f3051_row4_col12\" class=\"data row4 col12\" >0.01</td>\n",
       "      <td id=\"T_f3051_row4_col13\" class=\"data row4 col13\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row4_col14\" class=\"data row4 col14\" >0.04</td>\n",
       "      <td id=\"T_f3051_row4_col15\" class=\"data row4 col15\" >-0.10</td>\n",
       "      <td id=\"T_f3051_row4_col16\" class=\"data row4 col16\" >-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f3051_level0_row5\" class=\"row_heading level0 row5\" >Integrated Gradient (x Input)</th>\n",
       "      <td id=\"T_f3051_row5_col0\" class=\"data row5 col0\" >-0.02</td>\n",
       "      <td id=\"T_f3051_row5_col1\" class=\"data row5 col1\" >0.04</td>\n",
       "      <td id=\"T_f3051_row5_col2\" class=\"data row5 col2\" >0.30</td>\n",
       "      <td id=\"T_f3051_row5_col3\" class=\"data row5 col3\" >0.07</td>\n",
       "      <td id=\"T_f3051_row5_col4\" class=\"data row5 col4\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row5_col5\" class=\"data row5 col5\" >0.01</td>\n",
       "      <td id=\"T_f3051_row5_col6\" class=\"data row5 col6\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row5_col7\" class=\"data row5 col7\" >-0.01</td>\n",
       "      <td id=\"T_f3051_row5_col8\" class=\"data row5 col8\" >0.18</td>\n",
       "      <td id=\"T_f3051_row5_col9\" class=\"data row5 col9\" >0.03</td>\n",
       "      <td id=\"T_f3051_row5_col10\" class=\"data row5 col10\" >0.03</td>\n",
       "      <td id=\"T_f3051_row5_col11\" class=\"data row5 col11\" >0.03</td>\n",
       "      <td id=\"T_f3051_row5_col12\" class=\"data row5 col12\" >0.04</td>\n",
       "      <td id=\"T_f3051_row5_col13\" class=\"data row5 col13\" >0.04</td>\n",
       "      <td id=\"T_f3051_row5_col14\" class=\"data row5 col14\" >0.03</td>\n",
       "      <td id=\"T_f3051_row5_col15\" class=\"data row5 col15\" >0.10</td>\n",
       "      <td id=\"T_f3051_row5_col16\" class=\"data row5 col16\" >0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4b312f20a0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.show_table(bench.explain(data[\"Zusammenfassung\"][1][1], target=tag2id[\"Zusammenfassung\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d139da-fc35-43ee-8dfe-f4a45e28c16f",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0f48788e-1fcd-4e8c-9d7a-de40ac0ea363",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,\n",
       " [(23,\n",
       "   'MRE: VRE rektal B-DATE I-DATE I-DATE',\n",
       "   Evaluation(name='aopc_suff', score=-0.050719053)),\n",
       "  (30,\n",
       "   '- Wiedervorstellung zur nächsten Schrittmacherkontrolluntersuchung in unserer ORG am <[Pseudo] 05/11/2037> um 10 Uhr',\n",
       "   Evaluation(name='aopc_suff', score=0.09525423)),\n",
       "  (9, 'Externer Zuweiser: CPU', Evaluation(name='aopc_suff', score=0.3746343)),\n",
       "  (34,\n",
       "   'Wir empfehlen regelmäßige angiologische Kontrolluntersuchungen in der Praxis des betreuenden B-SALUTE B-TITLE B-PER',\n",
       "   Evaluation(name='aopc_suff', score=0.26184133)),\n",
       "  (8,\n",
       "   '- Kostaufbau nach Ernährungskonsil',\n",
       "   Evaluation(name='aopc_suff', score=0.55935806)),\n",
       "  (7,\n",
       "   '- Übernahme Herzchirurgie',\n",
       "   Evaluation(name='aopc_suff', score=0.8258164)),\n",
       "  (12,\n",
       "   '- Kaliumkontrolle und klinische Kontrolle morgen beim Hausarzt',\n",
       "   Evaluation(name='aopc_suff', score=0.37734532)),\n",
       "  (22,\n",
       "   '- Termin in der kardiologischen Ambulanz am B-DATE, 10:30 Uhr',\n",
       "   Evaluation(name='aopc_suff', score=0.24765943)),\n",
       "  (11,\n",
       "   '- AB weiter, regelmäßige Entzündungswertkontrollen',\n",
       "   Evaluation(name='aopc_suff', score=0.31902683)),\n",
       "  (6,\n",
       "   'Indikationen für stationären Aufenthalt:',\n",
       "   Evaluation(name='aopc_suff', score=0.42824212)),\n",
       "  (35,\n",
       "   '- Bei beschwerdefreiem Verlauf empfehlen wir eine kardiologische Kontrolluntersuchung in 6 Monaten, gerne auch bei einer niedergelassenen B-SALUTE oder Kollegen.',\n",
       "   Evaluation(name='aopc_suff', score=0.17013566)),\n",
       "  (14,\n",
       "   '-UNK- Ggf. Diskussion Peritonealdialyse',\n",
       "   Evaluation(name='aopc_suff', score=-0.37598774)),\n",
       "  (76,\n",
       "   '-UNK- Bitte kein Marcumar am Aufnahmetag einnehmen, ab INR < 2,0 Heparin-gesteuertes Bridging bis zur Implantation im stationären Kontext -LRB- kein prästationäres Bridging bei Z.n. Spontanblutung nach Clexane-Bridging -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=-0.44231477)),\n",
       "  (58,\n",
       "   '-UNK- Bei guter Verträglichkeit Evaluation einer Extension der antithrombotischen Therapie für > 12 Monate bei hohem Ischämierisiko und niedrigem Blutungsrisiko -LRB- Analog PEGASUS-Schema -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=-0.75033015)),\n",
       "  (7, 'Carotis-Doppler:', Evaluation(name='aopc_suff', score=-0.014803387)),\n",
       "  (14,\n",
       "   '-UNK- Bitte nüchtern sowie mindestens 48h Alkoholkarenz',\n",
       "   Evaluation(name='aopc_suff', score=-0.47030985)),\n",
       "  (9, 'unserer B-ORG I-ORG', Evaluation(name='aopc_suff', score=-0.13400139)),\n",
       "  (27,\n",
       "   'Carotis-Doppler -LRB- <[Pseudo] 03/2028> -RRB- :',\n",
       "   Evaluation(name='aopc_suff', score=-0.024299541)),\n",
       "  (42,\n",
       "   '- Amiodaron 200mg 1-0-0 in der Blanking Phase -LRB- bis zum <[Pseudo] 26/05/41> -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=-0.16930375)),\n",
       "  (3, 'Allergien:', Evaluation(name='aopc_suff', score=-0.0081550805)),\n",
       "  (11,\n",
       "   'Koronargefäße im Transplantat:',\n",
       "   Evaluation(name='aopc_suff', score=-0.2331898))])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eva_shap[\"Mix\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_shap[\"Mix\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8898f59f-4e12-4122-afb6-94473d267822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: - Kostaufbau nach Ernährungskonsil \tScore: 0.94\n",
      "Filtered: [('-', 0.018596554615035184), ('Kost', 0.4052477708745033), ('##aufbau', 0.0184040195059656), ('nach', 0.013779742537580682), ('Ernährung', 0.36493172497281945), ('##skon', 0.048083972730894244), ('[MASK]', -0.038987453409692736), ('##il', 0.0919686855660894)]\n",
      "\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "2 important token(s) only: 'Kost Ernährung' affects original score: 0.91 | Labeled: Befunde: 0.46513277292251587\n",
      "3 important token(s) only: 'Kost Ernährungil' affects original score: 0.93 | Labeled: Befunde: 0.8804022073745728\n",
      "4 important token(s) only: 'Kost Ernährungskonil' affects original score: 0.91 | Labeled: Medikation: 0.5377553105354309\n",
      "4 important token(s) only: 'Kost Ernährungskonil' affects original score: 0.91 | Labeled: Medikation: 0.5377553105354309\n",
      "5 important token(s) only: '- Kost Ernährungskonil' affects original score: 0.23 | Labeled: Mix: 0.7027848362922668\n",
      "6 important token(s) only: '- Kostaufbau Ernährungskonil' affects original score: 0.01 | Labeled: Mix: 0.9276393055915833\n",
      "6 important token(s) only: '- Kostaufbau Ernährungskonil' affects original score: 0.01 | Labeled: Mix: 0.9276393055915833\n",
      "7 important token(s) only: '- Kostaufbau nach Ernährungskonil' affects original score: -0.01 | Labeled: Mix: 0.9521846771240234\n",
      "\n",
      "Mean of all scores: 0.56\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b8c1c-2eda-4896-8f4c-63f63e59fbcc",
   "metadata": {},
   "source": [
    "### Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "74b2849c-dac8-445c-a5df-b3667092e177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: - Kostaufbau nach Ernährungskonsil \tScore: 0.94\n",
      "Filtered: [('-', 0.018596554615035184), ('Kost', 0.4052477708745033), ('##aufbau', 0.0184040195059656), ('nach', 0.013779742537580682), ('Ernährung', 0.36493172497281945), ('##skon', 0.048083972730894244), ('##s', -0.038987453409692736), ('##il', 0.0919686855660894)]\n",
      "\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "2 important token(s) only: 'Kost Ernährung' affects original score: 0.91 | Labeled: Befunde: 0.46513277292251587\n",
      "2 important token(s) only: 'Kost Ernährung' affects original score: 0.91 | Labeled: Befunde: 0.46513277292251587\n",
      "3 important token(s) only: 'Kost Ernährungil' affects original score: 0.93 | Labeled: Befunde: 0.8804022073745728\n",
      "4 important token(s) only: 'Kost Ernährungskonil' affects original score: 0.91 | Labeled: Medikation: 0.5377553105354309\n",
      "5 important token(s) only: 'Kost Ernährungskonsil' affects original score: 0.92 | Labeled: Befunde: 0.6901900768280029\n",
      "6 important token(s) only: '- Kost Ernährungskonsil' affects original score: 0.58 | Labeled: Befunde: 0.611973762512207\n",
      "6 important token(s) only: '- Kost Ernährungskonsil' affects original score: 0.58 | Labeled: Befunde: 0.611973762512207\n",
      "7 important token(s) only: '- Kostaufbau Ernährungskonsil' affects original score: 0.02 | Labeled: Mix: 0.9154998660087585\n",
      "8 important token(s) only: '- Kostaufbau nach Ernährungskonsil' affects original score: 0.0 | Labeled: Mix: 0.9375083446502686\n",
      "\n",
      "Mean of all scores: 0.67\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2a6b5-b1e8-426f-9b4a-06339cd682af",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens worsens score such that by including 's' & 'Röntgen' score is undermined** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434cefe-5815-4930-bff7-133d00363ecd",
   "metadata": {},
   "source": [
    "### 2.2 IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4a4c3313-86c3-4d9d-a8ba-a0aebe4b4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "EchoBefunde                          [[0.37, 0.01]]\n",
      "Befunde                              [[0.41, -0.19]]\n",
      "Medikation                           [[0.43, nan]]\n",
      "Zusammenfassung                      [[0.44, -0.54]]\n",
      "Abschluss                            [[0.45, 0.07]]\n",
      "Anamnese                             [[0.46, -0.02]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.6, 0.0]]\n",
      "Diagnosen                            [[0.66, nan]]\n",
      "Anrede                               [[0.71, 0.01]]\n",
      "KUBefunde                            [[0.77, 0.0]]\n",
      "Mix                                  [[0.8, 0.04]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k==\"IG\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "64c59619-d3da-4c82-a1c8-d3a86c250d57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,\n",
       " [(19,\n",
       "   'Echokardiographie vom <[Pseudo] 11/04/2032:>',\n",
       "   Evaluation(name='aopc_suff', score=0.9333624)),\n",
       "  (38,\n",
       "   'Kein relevanter Perikarderguss nachweisbar, V. cava inferior normal weit -LRB- 14 mm -RRB- und atemvariabel.',\n",
       "   Evaluation(name='aopc_suff', score=0.50635123)),\n",
       "  (45,\n",
       "   'V. cava inferior weiterhin erweitert auf 25 mm und nicht atemvariabel. Pleuraerguss rechts basal, ca. 4-5 cm Schichtdicke. Kein Pleuraerguss links.',\n",
       "   Evaluation(name='aopc_suff', score=0.5677398)),\n",
       "  (27,\n",
       "   'Untersuchung am Bett auf Kardio-Intensiv. Vorbekannt deutlich reduzierte Schallbedingungen, v.a. von parasternal.',\n",
       "   Evaluation(name='aopc_suff', score=0.32701594)),\n",
       "  (24,\n",
       "   'Transösophageale Echokardiographie vom <[Pseudo] 06/07/2036:>',\n",
       "   Evaluation(name='aopc_suff', score=0.17271966)),\n",
       "  (19,\n",
       "   'Echokardiographie vom <[Pseudo] 04/11/2037:>',\n",
       "   Evaluation(name='aopc_suff', score=0.5463312)),\n",
       "  (180,\n",
       "   'Deutlich dilatierter linker Ventrikel mit hochgradig verminderter systolischer Pumpfunktion bei Akinesie inferior und inferoseptal mediobasal und Hypokinesie der übrigen Wandabschnitte. Verminderte longitudinale Funktion des linken Ventrikels. Hinweis auf Relaxationsstörung des LV. LA deutlich dilatiert, RA leicht dilatiert, RV normal groß mit guter Pumpfunktion. Aortenwurzel normal weit. Kein Perikarderguss nachweisbar. Vena cava inferior nicht gestaut. Klappen dopplersonographisch und sonographisch regelrecht. Der systolische PA-Druck in Ruhe ist bei fehlender Trikuspidalklappeninsuffizienz nicht valide ableitbar. Erstbefund in domo.',\n",
       "   Evaluation(name='aopc_suff', score=0.15219034)),\n",
       "  (160,\n",
       "   'Fokussierte Fragestellung: Grenzwertig großer linker Ventrikel mit noch mittelgradig eingeschränkter systolischer Pumpfunktion bei globaler Hypokinesie und dyskoordiniertem Kontraktionsablauf. Eingeschränkte longitudinale Funktion des LV, erhaltene longitudinale LVFunktion des RV. LA, RA und RV normal groß, noch gute RV-Funktion. Kein Perikarderguss nachweisbar. Vena cava inferior nicht gestaut. Leichtgradige Mitralklappeninsuffizienz bei Anulusdilatation. Übrige Klappen sonographisch und dopplersonographisch regelrecht. Der systolische PA-Druck ist nicht ableitbar.',\n",
       "   Evaluation(name='aopc_suff', score=0.2210159)),\n",
       "  (306,\n",
       "   'Beurteilung Normal großer, septal leichtgradig hypertrophierter linker Ventrikel mit mittelgradiger systolischer Pumpfunktion bei globaler Hypokinesie, betont septal. Erhaltene longitudinale Funktion. Relaxationsstörung des LV. LA leichtgradig dilatiert, RA und RV normal groß, gute RVFunktion. Aortenwurzel normal weit. Aorta ascendens, soweit beurteilbar, normal weit. Kleine Perikardergusslamelle betont vor dem RA ohne hämodynamische Relevanz. Vena cava inferior nicht gestaut. Aortenklappe trikuspid mit guter Separation. Dopplersonographisch leichtgradiger Reflux über der Aortenklappe. Mittelgradige Mitralklappeninsuffizienz sowie leichtgradige Trikuspidalklappeninsuffizienz. Übrige Klappen sonographisch und dopplersonographisch regelrecht. Der systolische PA-Druck in Ruhe ist leichtgradig erhöht -LRB- ca. 40-45 mmHg -RRB- . Im Vergleich zur Voruntersuchung vom <[Pseudo] 09/08/2035:> Bei Z.n. Chemotherapie nun reduzierte LVEF. Zunahme der Mitralklappeninsuffizienz -LRB- aktuell mittelgradig -RRB- . Ansonsten idem.',\n",
       "   Evaluation(name='aopc_suff', score=0.09172816)),\n",
       "  (160,\n",
       "   'Normal großer, septal betont hypertrophierter linker Ventrikel mit guter Pumpfunktion ohne sicher abgrenzbare regionale Kontraktionsstörungen. Relaxationsstörung des LV. LA grenzwertig weit. RA und RV normal groß. Gute RV-Funktion. Kein Perikarderguss nachweisbar. Vena cava inferior nicht gestaut. Aorten- und Mitralklappe leicht verdickt mit guter Separation. Übrige Klappen sonographisch und dopplersonographisch regelrecht. Der systolische PA-Druck in Ruhe ist bei fehlender Trikuspidalinsuffizienz nicht sicher bestimmbar. Im Vergleich zur Voruntersuchung vom <[Pseudo] 15/10/2031:> Keine wesentliche Befundänderung.',\n",
       "   Evaluation(name='aopc_suff', score=0.17395838)),\n",
       "  (9, 'ml/min/1,73qm', Evaluation(name='aopc_suff', score=0.07642059)),\n",
       "  (6, '>60 s.u.', Evaluation(name='aopc_suff', score=-0.0060878)),\n",
       "  (7, 'Computertomographie:', Evaluation(name='aopc_suff', score=-0.11565066)),\n",
       "  (1, '92', Evaluation(name='aopc_suff', score=0.0)),\n",
       "  (7,\n",
       "   'leichtgradig einzustufen.',\n",
       "   Evaluation(name='aopc_suff', score=0.085573494)),\n",
       "  (33,\n",
       "   'Die ambulante Vorstellung erfolgte zur routinemäßigen kardiologischen Verlaufskontrolle. Subjektiv wird von einem klinisch stabilen kardialen Verlauf berichtet.',\n",
       "   Evaluation(name='aopc_suff', score=-0.00029296317)),\n",
       "  (13,\n",
       "   '-UNK- thrombogener Aortenbogen Grad II',\n",
       "   Evaluation(name='aopc_suff', score=0.0025895722))])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eva_ig[\"EchoBefunde\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_ig[\"EchoBefunde\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f30ef-2192-4742-a7a7-3184fbabded9",
   "metadata": {},
   "source": [
    "#### IG - Best Label: EchoBefunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e6d4bfd0-dc52-42bf-b532-2d515702d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Prediction swayed by removal of negative contributing tokens to label:  EchoBefunde\n",
      "Original sentence: Untersuchung am Bett auf Kardio-Intensiv. Vorbekannt deutlich reduzierte Schallbedingungen, v.a. von parasternal. \tScore: 0.98\n",
      "Filtered: [('Untersuchung', 0.09259950602257182), ('[MASK]', -0.04374447312125009), ('Bett', 0.0896032875732434), ('auf', 0.05463458143151262), ('Kard', 0.02933618097377146), ('[MASK]', -0.011090570313687558), ('-', 0.020506392251137623), ('[MASK]', -0.035195529380337806), ('[MASK]', -0.021597058514531794), ('[MASK]', -0.042405016230841475), ('##kann', 0.021550671515907862), ('##t', 0.012173796778867678), ('deutlich', 0.03536102810884739), ('[MASK]', -0.1213924959288616), ('[MASK]', -0.0037369807670049418), ('Schall', 0.017841558589441305), ('[MASK]', -0.10196366085702138), (',', 0.021705164255363778), ('v', 0.03520246030776116), ('.', 0.031911060834966955), ('[MASK]', -0.03222631493092209), ('[MASK]', -0.013086820152932591), ('von', 0.04546060741524235), ('[MASK]', -0.017486633759382773), ('[MASK]', -0.004300270830336496), ('[MASK]', -0.020759775117881137), ('[MASK]', -0.016457771580114078)]\n",
      "\n",
      "1 important token(s) only: 'Untersuchung' affects original score: 0.93 | Labeled: Befunde\n",
      "3 important token(s) only: 'Untersuchung Bett auf' affects original score: 0.33 | Labeled: EchoBefunde\n",
      "4 important token(s) only: 'Untersuchung Bett auf von' affects original score: 0.41 | Labeled: EchoBefunde\n",
      "5 important token(s) only: 'Untersuchung Bett auf deutlich von' affects original score: 0.26 | Labeled: EchoBefunde\n",
      "6 important token(s) only: 'Untersuchung Bett auf deutlich v von' affects original score: 0.3 | Labeled: EchoBefunde\n",
      "8 important token(s) only: 'Untersuchung Bett auf Kard deutlich v. von' affects original score: 0.23 | Labeled: EchoBefunde\n",
      "9 important token(s) only: 'Untersuchung Bett auf Kard deutlich, v. von' affects original score: 0.2 | Labeled: EchoBefunde\n",
      "10 important token(s) only: 'Untersuchung Bett auf Kardkann deutlich, v. von' affects original score: 0.23 | Labeled: EchoBefunde\n",
      "12 important token(s) only: 'Untersuchung Bett auf Kard -kann deutlich Schall, v. von' affects original score: 0.22 | Labeled: EchoBefunde\n",
      "13 important token(s) only: 'Untersuchung Bett auf Kard -kannt deutlich Schall, v. von' affects original score: 0.25 | Labeled: EchoBefunde\n",
      "\n",
      "Mean of all scores: 0.34\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}\"\"\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe025dd-0a80-471d-8918-2944d296f465",
   "metadata": {},
   "source": [
    "### Ablation Study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4180b806-759d-490f-9906-1e96de996eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Untersuchung am Bett auf Kardio-Intensiv. Vorbekannt deutlich reduzierte Schallbedingungen, v.a. von parasternal. \tScore: 0.98\n",
      "Filtered: [('Untersuchung', 0.09259950602257182), ('am', -0.04374447312125009), ('Bett', 0.0896032875732434), ('auf', 0.05463458143151262), ('Kard', 0.02933618097377146), ('##io', -0.011090570313687558), ('-', 0.020506392251137623), ('Intensiv', -0.035195529380337806), ('.', -0.021597058514531794), ('Vorbe', -0.042405016230841475), ('##kann', 0.021550671515907862), ('##t', 0.012173796778867678), ('deutlich', 0.03536102810884739), ('reduziert', -0.1213924959288616), ('##e', -0.0037369807670049418), ('Schall', 0.017841558589441305), ('##bedingungen', -0.10196366085702138), (',', 0.021705164255363778), ('v', 0.03520246030776116), ('.', 0.031911060834966955), ('a', -0.03222631493092209), ('.', -0.013086820152932591), ('von', 0.04546060741524235), ('par', -0.017486633759382773), ('##aster', -0.004300270830336496), ('##nal', -0.020759775117881137), ('.', -0.016457771580114078)]\n",
      "\n",
      "3 important token(s) only: 'Untersuchung reduziertbedingungen' affects original score: 0.38 | Labeled: EchoBefunde\n",
      "5 important token(s) only: 'Untersuchung Bett auf reduziertbedingungen' affects original score: 0.17 | Labeled: EchoBefunde\n",
      "8 important token(s) only: 'Untersuchung am Bett auf Vorbe reduziertbedingungen von' affects original score: 0.08 | Labeled: EchoBefunde\n",
      "11 important token(s) only: 'Untersuchung am Bett auf Intensiv Vorbe deutlich reduziertbedingungen v von' affects original score: 0.08 | Labeled: EchoBefunde\n",
      "14 important token(s) only: 'Untersuchung am Bett auf Kard Intensiv Vorbe deutlich reduziertbedingungen v. a von' affects original score: 0.05 | Labeled: EchoBefunde\n",
      "16 important token(s) only: 'Untersuchung am Bett auf Kard Intensiv. Vorbe deutlich reduziertbedingungen, v. a von' affects original score: 0.02 | Labeled: EchoBefunde\n",
      "19 important token(s) only: 'Untersuchung am Bett auf Kard - Intensiv. Vorbekann deutlich reduziertbedingungen, v. a vonnal' affects original score: 0.01 | Labeled: EchoBefunde\n",
      "22 important token(s) only: 'Untersuchung am Bett auf Kard - Intensiv. Vorbekann deutlich reduziert Schallbedingungen, v. a von parnal.' affects original score: 0.0 | Labeled: EchoBefunde\n",
      "24 important token(s) only: 'Untersuchung am Bett auf Kard - Intensiv. Vorbekannt deutlich reduziert Schallbedingungen, v. a. von parnal.' affects original score: 0.0 | Labeled: EchoBefunde\n",
      "27 important token(s) only: 'Untersuchung am Bett auf Kardio - Intensiv. Vorbekannt deutlich reduzierte Schallbedingungen, v. a. von parasternal.' affects original score: 0.0 | Labeled: EchoBefunde\n",
      "\n",
      "Mean of all scores: 0.08\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}\"\"\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5be4a7-3a77-451f-8f1d-a331ecadf136",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens in first step improves score substantially such that correct label is predicted** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641c64-5e07-4e86-8198-01ae7c16a030",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ec9857fb-b607-438a-83ce-b256a042bf2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,\n",
       " [(23,\n",
       "   'MRE: VRE rektal B-DATE I-DATE I-DATE',\n",
       "   Evaluation(name='aopc_suff', score=-0.00085239566)),\n",
       "  (30,\n",
       "   '- Wiedervorstellung zur nächsten Schrittmacherkontrolluntersuchung in unserer ORG am <[Pseudo] 05/11/2037> um 10 Uhr',\n",
       "   Evaluation(name='aopc_suff', score=0.9543541)),\n",
       "  (9, 'Externer Zuweiser: CPU', Evaluation(name='aopc_suff', score=0.9800445)),\n",
       "  (34,\n",
       "   'Wir empfehlen regelmäßige angiologische Kontrolluntersuchungen in der Praxis des betreuenden B-SALUTE B-TITLE B-PER',\n",
       "   Evaluation(name='aopc_suff', score=0.98169893)),\n",
       "  (8,\n",
       "   '- Kostaufbau nach Ernährungskonsil',\n",
       "   Evaluation(name='aopc_suff', score=0.9295922)),\n",
       "  (7,\n",
       "   '- Übernahme Herzchirurgie',\n",
       "   Evaluation(name='aopc_suff', score=0.9663508)),\n",
       "  (12,\n",
       "   '- Kaliumkontrolle und klinische Kontrolle morgen beim Hausarzt',\n",
       "   Evaluation(name='aopc_suff', score=0.4126138)),\n",
       "  (22,\n",
       "   '- Termin in der kardiologischen Ambulanz am B-DATE, 10:30 Uhr',\n",
       "   Evaluation(name='aopc_suff', score=0.22014639)),\n",
       "  (11,\n",
       "   '- AB weiter, regelmäßige Entzündungswertkontrollen',\n",
       "   Evaluation(name='aopc_suff', score=0.9641853)),\n",
       "  (6,\n",
       "   'Indikationen für stationären Aufenthalt:',\n",
       "   Evaluation(name='aopc_suff', score=0.9166217)),\n",
       "  (35,\n",
       "   '- Bei beschwerdefreiem Verlauf empfehlen wir eine kardiologische Kontrolluntersuchung in 6 Monaten, gerne auch bei einer niedergelassenen B-SALUTE oder Kollegen.',\n",
       "   Evaluation(name='aopc_suff', score=0.712466)),\n",
       "  (14,\n",
       "   '-UNK- Ggf. Diskussion Peritonealdialyse',\n",
       "   Evaluation(name='aopc_suff', score=0.04730726)),\n",
       "  (76,\n",
       "   '-UNK- Bitte kein Marcumar am Aufnahmetag einnehmen, ab INR < 2,0 Heparin-gesteuertes Bridging bis zur Implantation im stationären Kontext -LRB- kein prästationäres Bridging bei Z.n. Spontanblutung nach Clexane-Bridging -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=0.014594844)),\n",
       "  (58,\n",
       "   '-UNK- Bei guter Verträglichkeit Evaluation einer Extension der antithrombotischen Therapie für > 12 Monate bei hohem Ischämierisiko und niedrigem Blutungsrisiko -LRB- Analog PEGASUS-Schema -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=0.024267798)),\n",
       "  (7, 'Carotis-Doppler:', Evaluation(name='aopc_suff', score=-0.002852333)),\n",
       "  (14,\n",
       "   '-UNK- Bitte nüchtern sowie mindestens 48h Alkoholkarenz',\n",
       "   Evaluation(name='aopc_suff', score=0.023239456)),\n",
       "  (9,\n",
       "   'unserer B-ORG I-ORG',\n",
       "   Evaluation(name='aopc_suff', score=0.00011058261)),\n",
       "  (27,\n",
       "   'Carotis-Doppler -LRB- <[Pseudo] 03/2028> -RRB- :',\n",
       "   Evaluation(name='aopc_suff', score=-0.0041260566)),\n",
       "  (42,\n",
       "   '- Amiodaron 200mg 1-0-0 in der Blanking Phase -LRB- bis zum <[Pseudo] 26/05/41> -RRB-',\n",
       "   Evaluation(name='aopc_suff', score=0.3681573)),\n",
       "  (3, 'Allergien:', Evaluation(name='aopc_suff', score=-0.0013190906)),\n",
       "  (11,\n",
       "   'Koronargefäße im Transplantat:',\n",
       "   Evaluation(name='aopc_suff', score=-0.0031571991))])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eva_ig[\"Mix\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_ig[\"Mix\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7bd4fdbf-95cf-4643-beea-e0fe755bcd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Prediction swayed by removal of negative contributing tokens to label:  Befunde\n",
      "Original sentence: - Kostaufbau nach Ernährungskonsil \tScore: 0.94\n",
      "Filtered: [('[MASK]', -0.17239096756130623), ('Kost', 0.28357038194503126), ('##aufbau', 0.027203998969917657), ('[MASK]', -0.08204218859432016), ('Ernährung', 0.02043245855163943), ('##skon', 0.16957146868890877), ('##s', 0.19929151477819718), ('##il', 0.0013163738851477715)]\n",
      "\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "2 important token(s) only: 'Kosts' affects original score: 0.93 | Labeled: Befunde: 0.6500313878059387\n",
      "2 important token(s) only: 'Kosts' affects original score: 0.93 | Labeled: Befunde: 0.6500313878059387\n",
      "3 important token(s) only: 'Kostskons' affects original score: 0.92 | Labeled: Befunde: 0.6947952508926392\n",
      "4 important token(s) only: 'Kostaufbauskons' affects original score: 0.94 | Labeled: Befunde: 0.9438266158103943\n",
      "4 important token(s) only: 'Kostaufbauskons' affects original score: 0.94 | Labeled: Befunde: 0.9438266158103943\n",
      "5 important token(s) only: 'Kostaufbau Ernährungskons' affects original score: 0.93 | Labeled: Befunde: 0.9179874658584595\n",
      "5 important token(s) only: 'Kostaufbau Ernährungskons' affects original score: 0.93 | Labeled: Befunde: 0.9179874658584595\n",
      "6 important token(s) only: 'Kostaufbau Ernährungskonsil' affects original score: 0.93 | Labeled: Befunde: 0.9350565075874329\n",
      "\n",
      "Mean of all scores: 0.93\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88e948-3f9f-4d68-aefe-af474b4caa05",
   "metadata": {},
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d5de7268-2570-4b47-8339-cde0537b36f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: - Kostaufbau nach Ernährungskonsil \tScore: 0.94\n",
      "Filtered: [('-', -0.17239096756130623), ('Kost', 0.28357038194503126), ('##aufbau', 0.027203998969917657), ('nach', -0.08204218859432016), ('Ernährung', 0.02043245855163943), ('##skon', 0.16957146868890877), ('##s', 0.19929151477819718), ('##il', 0.0013163738851477715)]\n",
      "\n",
      "1 important token(s) only: 'Kost' affects original score: 0.94 | Labeled: Zusammenfassung: 0.48217278718948364\n",
      "2 important token(s) only: 'Kosts' affects original score: 0.93 | Labeled: Befunde: 0.6500313878059387\n",
      "2 important token(s) only: 'Kosts' affects original score: 0.93 | Labeled: Befunde: 0.6500313878059387\n",
      "3 important token(s) only: '- Kosts' affects original score: 0.91 | Labeled: Befunde: 0.9212023019790649\n",
      "4 important token(s) only: '- Kostskons' affects original score: 0.9 | Labeled: Diagnosen: 0.7408345341682434\n",
      "5 important token(s) only: '- Kost nachskons' affects original score: 0.88 | Labeled: Befunde: 0.8905332088470459\n",
      "6 important token(s) only: '- Kostaufbau nachskons' affects original score: 0.73 | Labeled: Befunde: 0.7503605484962463\n",
      "6 important token(s) only: '- Kostaufbau nachskons' affects original score: 0.73 | Labeled: Befunde: 0.7503605484962463\n",
      "7 important token(s) only: '- Kostaufbau nach Ernährungskons' affects original score: 0.0 | Labeled: Mix: 0.9369111657142639\n",
      "8 important token(s) only: '- Kostaufbau nach Ernährungskonsil' affects original score: 0.0 | Labeled: Mix: 0.9375083446502686\n",
      "\n",
      "Mean of all scores: 0.66\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ####\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))])\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005353da-2ccf-4eb8-af24-d2758bb86e69",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens improves score throughout last half of total steps such that correct label is predicted in last two of them and beforehand scores for false labels are reduced** </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIEdeep",
   "language": "python",
   "name": "miedeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
