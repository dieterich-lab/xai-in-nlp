{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d971da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which of the interpretability methods is more faithful ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d5d2c",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "Calculate for IG & SHAP each Comprehensiveness & Sufficiency\n",
    "* Which of them performs better in each ?\n",
    "    - Reasons?\n",
    "* Which labels are conspicuous for scoring high/low or \n",
    "    - Metric specific tendencies ?\n",
    "    - Data Bias ?\n",
    "* Finally IG or SHAP \"better\" overall ?\n",
    "    - Pros/Cons of approach with Faithfulness metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a27c1",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Comprehensiveness](#comp)  \n",
    "    1.1 [SHAP](#comp_shap)  \n",
    "    1.2 [IG](#comp_ig)\n",
    "2. [Additional Study](#comp_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c9f4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ffdb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {0: 'Anrede', 1: 'Diagnosen', 2: 'AllergienUnverträglichkeitenRisiken', 3: 'Anamnese', 4: 'Medikation', 5: 'KUBefunde', 6: 'Befunde', 7: 'EchoBefunde', 8: 'Zusammenfassung', 9: 'Mix', 10: 'Abschluss'}\n",
    "tag2id = {tag: id for id, tag in id2tag.items()}\n",
    "\n",
    "labels = list(id2tag.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9bfdb5-62f3-496f-a926-af8c11a2da66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37755405-81e9-4d17-aafc-6c53da19daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ig.p\", \"rb\") as f:\n",
    "    ig = pickle.load(f)\n",
    "    \n",
    "with open(\"shap.p\", \"rb\") as f:\n",
    "    shap = pickle.load(f)\n",
    "\n",
    "with open(\"eva_ig.p\", \"rb\") as f:\n",
    "    eva_ig = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_shap.p\", \"rb\") as f:\n",
    "    eva_shap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d76893b-cd29-4866-a8d3-c38f0db69e4c",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anrede [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10]\n",
      "Diagnosen [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "AllergienUnverträglichkeitenRisiken [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12]\n",
      "Anamnese [0, 1, 2, 3, 4, 6, 9, 10, 11, 12] [5, 7, 8, 13]\n",
      "Medikation [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "KUBefunde [0, 1, 2, 3, 5, 6, 7, 8, 9, 10] [4]\n",
      "Befunde [0, 1, 2, 3, 4, 5, 6, 8, 9, 10] [7, 11]\n",
      "EchoBefunde [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12, 13, 14, 15, 16]\n",
      "Zusammenfassung [0, 1, 2, 3, 4, 6, 7, 8, 9, 10] [5, 11, 12]\n",
      "Mix [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Abschluss [0, 2, 3, 4, 5, 6, 7, 8, 9, 11] [1, 10, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "compr, suff = {l: None for l in labels}, {l: None for l in labels}\n",
    "\n",
    "for l in labels:\n",
    "    right = [data[l].index(d) for d in data[l] if d[0] == l]\n",
    "    wrong = [data[l].index(d) for d in data[l] if d[0] != l]\n",
    "    print(l, right, wrong)\n",
    "    compr_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    compr_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    \n",
    "    suff_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    suff_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    \n",
    "    compr[l] = {\"IG\": (np.nanmean([e for e in compr_ig if compr_ig.index(e) in right]), np.nanmean([e for e in compr_ig if compr_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in compr_shap if compr_shap.index(e) in right]), np.nanmean([e for e in compr_shap if compr_shap.index(e) in wrong]))}\n",
    "    suff[l] = {\"IG\": (np.nanmean([e for e in suff_ig if suff_ig.index(e) in right]), np.nanmean([e for e in suff_ig if suff_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in suff_shap if suff_shap.index(e) in right]), np.nanmean([e for e in suff_shap if suff_shap.index(e) in wrong]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b251bf-3e57-4bd1-8142-03ad059f6f5c",
   "metadata": {},
   "source": [
    "## **1. Comprehensiveness**<a name=\"comp\"></a>\n",
    "$f(x)_j-f(x\\text\\r_j)_j$ → Score difference once most important tokens are removed.   \n",
    "**Higher scores better: Exclusion of top 10-100% most important tokens should harm prediction.**  \n",
    "We expect performance on sentence to suddenly drop once 10% significant tokens removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bc2c0-07fb-4512-a63c-b051c42483dd",
   "metadata": {},
   "source": [
    "### **1.1. SHAP**<a name=\"comp_shap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c10f582-b5a2-4431-9f2f-450f16cefb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Comprehensiveness mean scores\n",
      "-----------------------------------  -------------------------------\n",
      "Anrede                               [[1.0, nan]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.85, nan]]\n",
      "Mix                                  [[0.85, 0.99]]\n",
      "Zusammenfassung                      [[0.82, 0.12]]\n",
      "KUBefunde                            [[0.76, 0.93]]\n",
      "Diagnosen                            [[0.75, nan]]\n",
      "EchoBefunde                          [[0.66, nan]]\n",
      "Anamnese                             [[0.65, 0.62]]\n",
      "Befunde                              [[0.65, 0.79]]\n",
      "Medikation                           [[0.64, nan]]\n",
      "Abschluss                            [[0.58, 0.86]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in compr[l].items() if k == \"SHAP\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = True)\n",
    "table.insert(0, [\"Label\", \"Comprehensiveness mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e5b26-45af-4db2-904c-d966dcbf6794",
   "metadata": {},
   "source": [
    "### Manual Comprehensiveness Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e828783-a7b6-475e-aef2-bfedda3b9faa",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Anrede<a name=\"comp_shap_An\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe06e21a-181f-414a-a590-94280805a77a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Anrede\"][9][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Anrede\"]\n",
    "metr = eva_shap[\"Anrede\"][9].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0][:sect]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = shap[\"Anrede\"][i] #score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}: {np.max(list(bench.score(sentence).values()))}\")\n",
    "    aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af1866",
   "metadata": {},
   "source": [
    "#### IG: Anrede<a name=\"comp_ig_An\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce78e1cc-58d9-4a32-8458-a4fd97ec45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.51\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Anrede\"][9][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Anrede\"]\n",
    "metr = eva_ig[\"Anrede\"][9].explanation #bench.explain(sent, target=target)[0] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0][:sect]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Anrede\"][i] #score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}: {np.max(list(bench.score(sentence).values()))}\")\n",
    "    aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc501ff9-a1ec-4fca-98a3-dc56695cd96c",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Abschluss<a name=\"comp_shap_Ab\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe21ae8-d49e-418c-b065-2a20cc8d6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.86\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Abschluss\"][2][1]\n",
    "target = tag2id[\"Abschluss\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_shap[\"Abschluss\"][1].explanation #bench.explain(sent, target=target)[4] ### SHAP ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = shap[\"Abschluss\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff16ae",
   "metadata": {},
   "source": [
    "#### IG : Abschluss<a name=\"comp_ig_Ab\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1034f3eb-e58e-480f-8b1a-f90c9f9a5a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.06\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Abschluss\"][2][1]\n",
    "target = tag2id[\"Abschluss\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_ig[\"Abschluss\"][0].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Abschluss\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db416662-d948-4678-8c9b-07df342f37aa",
   "metadata": {},
   "source": [
    "### **1.2. Integrated Gradients**<a name=\"comp_ig\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f69b242-a764-4871-adf7-9bbb9e0a1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Comprehensiveness mean scores\n",
      "-----------------------------------  -------------------------------\n",
      "Mix                                  [[0.7, 0.17]]\n",
      "Anrede                               [[0.4, nan]]\n",
      "Abschluss                            [[0.33, 0.06]]\n",
      "Diagnosen                            [[0.31, nan]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.31, nan]]\n",
      "KUBefunde                            [[0.29, 0.38]]\n",
      "EchoBefunde                          [[0.25, nan]]\n",
      "Befunde                              [[0.22, 0.0]]\n",
      "Medikation                           [[0.13, nan]]\n",
      "Zusammenfassung                      [[0.13, 0.26]]\n",
      "Anamnese                             [[-0.0, 0.07]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in compr[l].items() if k == \"IG\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = True)\n",
    "table.insert(0, [\"Label\", \"Comprehensiveness mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5599a34-f7bc-4d67-86a1-3fa809f459f1",
   "metadata": {},
   "source": [
    "#### IG - Best Label: Mix<a name=\"comp_ig_Mi\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3f00a6-76ee-4049-8423-d07a23a44a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.91\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "target = tag2id[\"Mix\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_ig[\"Mix\"][0].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Mix\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59f128-550d-4c05-8c31-bf77e280f9d2",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Anamnese<a name=\"comp_ig_Ana\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03fccbf2-bc08-4fe9-9608-da1ef1d797b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Anamnese\"][1][1]\n",
    "target = tag2id[\"Anamnese\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_ig[\"Anamnese\"][0].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Anamnese\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ffaae-9a23-4a5f-91f8-1eb33ebcc9a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce201a45-d644-4643-b572-0c16f55b9d98",
   "metadata": {},
   "source": [
    "## Additional Study: Include negative contributing tokens<a name=\"comp_add\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568f525-7904-43f8-9ecb-c922797b10aa",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Anrede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774ec089-46d2-4b0d-959f-89336489bff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Anrede\"][9][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Anrede\"]\n",
    "metr = eva_shap[\"Anrede\"][9].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0][:sect]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = shap[\"Anrede\"][\"Add\"][i] #score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}: {np.max(list(bench.score(sentence).values()))}\")\n",
    "    aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d89d9-4eff-41e2-a48d-06a7f16a17e5",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens doesn't improve score** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9cf2a-9c5b-4b17-9572-9935de57b0b0",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Anrede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9d9a452-1bb0-4914-bf32-ce9fdf0f07f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.59\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Abschluss\"][2][1]\n",
    "target = tag2id[\"Abschluss\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_shap[\"Abschluss\"][1].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = shap[\"Abschluss\"][\"Add\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88891a2b-2474-49a5-9c1b-013f1375a6ea",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens has disadvantage: Doesn't stray model that much away from right prediction, such that correct label in upper half is sometimes predicted** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dea85-fc7e-49f5-a949-238a4526876d",
   "metadata": {},
   "source": [
    "#### IG - Best Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d3c715d-6361-4a20-88cf-ca6886e61f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.93\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "target = tag2id[\"Mix\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_ig[\"Mix\"][0].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Mix\"][\"Add\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0addea9e-a54f-49ed-986d-3fabd5c25461",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens has no siginificant effect** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996a6b3",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Anamnese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c23e88f-7426-401c-b632-b00d5670e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.46\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Anamnese\"][1][1]\n",
    "target = tag2id[\"Anamnese\"]\n",
    "score = 1.0 #bench.score(sent)\n",
    "metr = eva_ig[\"Anamnese\"][2].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = list(metr.scores[1:-1])\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, scores)]}\\n\")\n",
    "\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len([s for s in scores if s>=0])*i)\n",
    "    sor_scores = np.sort(scores)[::-1][:sect]\n",
    "    sentence = metr.tokens[1:-1]\n",
    "    real_i = [scores.index(s) for s in sor_scores if s>=0]\n",
    "    for x in real_i:\n",
    "        sentence[x] = \"[MASK]\"\n",
    "    sentence = list(filter(lambda x: x!= \"[MASK]\", sentence))\n",
    "    #sentence = tokenizer.convert_tokens_to_string(sentence)\n",
    "    new = ig[\"Anamnese\"][\"Add\"][i]# score[f\"LABEL_{target}\"] - bench.score(sentence)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) removed: {sentence} \\t affects original sentence score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(sentence).values()))]}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b551d7-b2f1-4233-8326-7cc54f7ba53a",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens improves score such that Anamnese loses probability in upper half and false positives are predicted in lower half** </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
