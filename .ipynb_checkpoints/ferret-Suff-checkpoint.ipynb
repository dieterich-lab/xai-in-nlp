{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a236abef-0f4e-483b-8fc5-c8eda47986ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ferret import Benchmark\n",
    "from statistics import mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3806a3-b861-4391-8d21-411a3bbe07bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4536ba06-8bed-4ae4-80ef-aaec59f3c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa62c9a-5903-49a1-a1e4-6ed674a4d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"/beegfs/scratch/rsari/BertSeqCA/checkpoint-10000/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b10116-de23-4fb6-98bb-3e078a38dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {0: 'Anrede', 1: 'Diagnosen', 2: 'AllergienUnverträglichkeitenRisiken', 3: 'Anamnese', 4: 'Medikation', 5: 'KUBefunde', 6: 'Befunde', 7: 'EchoBefunde', 8: 'Zusammenfassung', 9: 'Mix', 10: 'Abschluss'}\n",
    "tag2id = {tag: id for id, tag in id2tag.items()}\n",
    "\n",
    "labels = list(id2tag.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda40532-e9c5-45da-a68b-719c4e0a4484",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which of the interpretability methods is more faithful ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576f42-0c55-41cf-b5fc-134fb15f6296",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "Calculate for IG & SHAP each Comprehensiveness & Sufficiency\n",
    "* Which of them performs better in each ?\n",
    "    - Reasons?\n",
    "* Which labels are conspicious for scoring high/low or \n",
    "    - Metric specific tendencies ?\n",
    "    - Data Bias ?\n",
    "* Finally IG or SHAP \"better\" overall ?\n",
    "    - Pros/Cons of approach with Faithfulness metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef02a0e4-0eb9-4448-943d-bb763edd8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = Benchmark(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c243d98-cb40-4d8f-a55c-29b278e3da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "[(k, len(v)) for k,v in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89cf5343-bf9e-4633-aa50-2214a66067a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ig.p\", \"rb\") as f:\n",
    "    ig = pickle.load(f)\n",
    "    \n",
    "with open(\"shap.p\", \"rb\") as f:\n",
    "    shap = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_ig.p\", \"rb\") as f:\n",
    "    eva_ig = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_shap.p\", \"rb\") as f:\n",
    "    eva_shap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6243f-5f00-4101-92ef-f719e8d1707a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compr, suff = {l: None for l in [\"Anrede\"]}, {l: None for l in [\"Anrede\"]}\n",
    "\n",
    "for l in labels:\n",
    "    right = [data[l].index(d) for d in data[l] if d[0] == l]\n",
    "    wrong = [data[l].index(d) for d in data[l] if d[0] != l]\n",
    "    print(l, right, wrong)\n",
    "    compr_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    compr_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    \n",
    "    suff_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    suff_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    \n",
    "    compr[l] = {\"IG\": (np.nanmean([e for e in compr_ig if compr_ig.index(e) in right]), np.nanmean([e for e in compr_ig if compr_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in compr_shap if compr_shap.index(e) in right]), np.nanmean([e for e in compr_shap if compr_shap.index(e) in wrong]))}\n",
    "    suff[l] = {\"IG\": (np.nanmean([e for e in suff_ig if suff_ig.index(e) in right]), np.nanmean([e for e in suff_ig if suff_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in suff_shap if suff_shap.index(e) in right]), np.nanmean([e for e in suff_shap if suff_shap.index(e) in wrong]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b16b07-ef35-40b7-a4c4-f7b8b1d77cf4",
   "metadata": {},
   "source": [
    "## **2. Sufficiency**\n",
    "$f(x)_j - f(r_j)_j$ → Score difference once most important tokens included.  \n",
    "**Lower scores better: Inclusion of top 10-100% most important tokens should drive the prediction.**  \n",
    "Measures if top k% (10 step: 10-100) tokens in explanation are sufficient for the right prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7a3eb-930f-4349-96b6-c1dd84d5bd8e",
   "metadata": {},
   "source": [
    "### **2.1. SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a9df8f-9a65-4fcb-b79a-1aef1dd4bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "Zusammenfassung                      [[0.0, 0.01]]\n",
      "Anamnese                             [[0.02, 0.05]]\n",
      "Befunde                              [[0.02, 0.02]]\n",
      "EchoBefunde                          [[0.04, nan]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.06, nan]]\n",
      "Medikation                           [[0.07, nan]]\n",
      "Anrede                               [[0.1, nan]]\n",
      "Abschluss                            [[0.13, 0.35]]\n",
      "Diagnosen                            [[0.19, nan]]\n",
      "KUBefunde                            [[0.26, 0.12]]\n",
      "Mix                                  [[0.4, 0.1]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k == \"SHAP\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf7238-b568-4af9-93dd-3125c728ce45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eva_shap[\"Zusammenfassung\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_shap[\"Zusammenfassung\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9978b-2448-4987-a72a-89e8b683fe17",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726f325e-d511-43cb-b60c-e702f3c18e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: -0.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e19545b-ffe1-42d9-91ab-8a8115a700b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.99\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target, normalize_scores=True)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d139da-fc35-43ee-8dfe-f4a45e28c16f",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48788e-1fcd-4e8c-9d7a-de40ac0ea363",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eva_shap[\"Mix\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_shap[\"Mix\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8898f59f-4e12-4122-afb6-94473d267822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.56\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434cefe-5815-4930-bff7-133d00363ecd",
   "metadata": {},
   "source": [
    "### **2.2 Integrated Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4a4c3313-86c3-4d9d-a8ba-a0aebe4b4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "EchoBefunde                          [[0.37, 0.01]]\n",
      "Befunde                              [[0.41, -0.19]]\n",
      "Medikation                           [[0.43, nan]]\n",
      "Zusammenfassung                      [[0.44, -0.54]]\n",
      "Abschluss                            [[0.45, 0.07]]\n",
      "Anamnese                             [[0.46, -0.02]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.6, 0.0]]\n",
      "Diagnosen                            [[0.66, nan]]\n",
      "Anrede                               [[0.71, 0.01]]\n",
      "KUBefunde                            [[0.77, 0.0]]\n",
      "Mix                                  [[0.8, 0.04]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k==\"IG\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c59619-d3da-4c82-a1c8-d3a86c250d57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eva_ig[\"EchoBefunde\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_ig[\"EchoBefunde\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f30ef-2192-4742-a7a7-3184fbabded9",
   "metadata": {},
   "source": [
    "#### IG - Best Label: EchoBefunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d4bfd0-dc52-42bf-b532-2d515702d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.34\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}\"\"\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641c64-5e07-4e86-8198-01ae7c16a030",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9857fb-b607-438a-83ce-b256a042bf2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eva_ig[\"Mix\"]), [(len(eva.explanation.tokens[1:-1]), eva.explanation.text, eva.evaluation_scores[1]) for eva in eva_ig[\"Mix\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd4fdbf-95cf-4643-beea-e0fe755bcd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.93\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfeb56-d4bc-4df9-a0b5-c23d5cffb429",
   "metadata": {},
   "source": [
    "## 3.1 Additional Study: Include negative contributing tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253590d-3900-4358-a248-7abd8a6c7075",
   "metadata": {},
   "source": [
    "### Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e354500-6304-4275-a03d-b14d63ccbfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.02\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ccde5-3147-4e2f-bf87-43e0ccddf424",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens has no significant effect** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54391eb-f7cf-4fe2-89de-a210d9c56b19",
   "metadata": {},
   "source": [
    "### Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b2849c-dac8-445c-a5df-b3667092e177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.65\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a21f52-2efb-4899-aa77-7b777a3024fe",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens worsens score** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5de7268-2570-4b47-8339-cde0537b36f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.66\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ####\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd63c9e-7afb-4b71-9f83-04a6c81c6613",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens improves score throughout last half of total steps such that correct label is predicted in last two of them and beforehand scores for false labels are reduced** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fce575-c881-4bce-bf8a-712014ca46a8",
   "metadata": {},
   "source": [
    "### EchoBefunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4180b806-759d-490f-9906-1e96de996eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Explainer:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.08\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = [abs(i) for i in metr.scores[1:-1]]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "try:\n",
    "    assert score[f\"LABEL_{target}\"] <= bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens))))[f\"LABEL_{target}\"]\n",
    "except:\n",
    "    #print(\"! Prediction swayed by removal of negative contributing tokens to label: \", \n",
    "          id2tag[np.argmax(list(bench.score(tokenizer.convert_tokens_to_string(list(filter(lambda x:x!=\"[MASK]\", tokens)))).values()))]\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}\"\"\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c399a7-74a1-4278-aee7-911e921b8010",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens in first step improves score substantially such that correct label is predicted** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f465a-eead-4d0e-b0b0-9f43aedc6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.show_table(bench.explain(data[\"Zusammenfassung\"][1][1], target=tag2id[\"Zusammenfassung\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIEdeep",
   "language": "python",
   "name": "miedeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
