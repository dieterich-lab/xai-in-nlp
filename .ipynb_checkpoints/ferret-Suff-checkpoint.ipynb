{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda40532-e9c5-45da-a68b-719c4e0a4484",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which of the interpretability methods is more faithful ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576f42-0c55-41cf-b5fc-134fb15f6296",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "Calculate for IG & SHAP each Comprehensiveness & Sufficiency\n",
    "* Which of them performs better in each ?\n",
    "    - Reasons?\n",
    "* Which labels are conspicuous for scoring high/low or \n",
    "    - Metric specific tendencies ?\n",
    "    - Data Bias ?\n",
    "* Finally IG or SHAP \"better\" overall ?\n",
    "    - Pros/Cons of approach with Faithfulness metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c0407",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "2. [Sufficiency](#suff)  \n",
    "    2.1 [SHAP](#suff_shap)  \n",
    "    2.2 [IG](#suff_ig)\n",
    "3. [Additional Study](#suff_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c825b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b10116-de23-4fb6-98bb-3e078a38dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {0: 'Anrede', 1: 'Diagnosen', 2: 'AllergienUnverträglichkeitenRisiken', 3: 'Anamnese', 4: 'Medikation', 5: 'KUBefunde', 6: 'Befunde', 7: 'EchoBefunde', 8: 'Zusammenfassung', 9: 'Mix', 10: 'Abschluss'}\n",
    "tag2id = {tag: id for id, tag in id2tag.items()}\n",
    "\n",
    "labels = list(id2tag.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c243d98-cb40-4d8f-a55c-29b278e3da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf5343-bf9e-4633-aa50-2214a66067a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"ig.p\", \"rb\") as f:\n",
    "    ig = pickle.load(f)\n",
    "    \n",
    "with open(\"shap.p\", \"rb\") as f:\n",
    "    shap = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_ig.p\", \"rb\") as f:\n",
    "    eva_ig = pickle.load(f)\n",
    "    \n",
    "with open(\"eva_shap.p\", \"rb\") as f:\n",
    "    eva_shap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf6243f-5f00-4101-92ef-f719e8d1707a",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anrede [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10]\n",
      "Diagnosen [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "AllergienUnverträglichkeitenRisiken [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12]\n",
      "Anamnese [0, 1, 2, 3, 4, 6, 9, 10, 11, 12] [5, 7, 8, 13]\n",
      "Medikation [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] []\n",
      "KUBefunde [0, 1, 2, 3, 5, 6, 7, 8, 9, 10] [4]\n",
      "Befunde [0, 1, 2, 3, 4, 5, 6, 8, 9, 10] [7, 11]\n",
      "EchoBefunde [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [10, 11, 12, 13, 14, 15, 16]\n",
      "Zusammenfassung [0, 1, 2, 3, 4, 6, 7, 8, 9, 10] [5, 11, 12]\n",
      "Mix [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Abschluss [0, 2, 3, 4, 5, 6, 7, 8, 9, 11] [1, 10, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n",
      "Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "compr, suff = {l: None for l in [\"Anrede\"]}, {l: None for l in [\"Anrede\"]}\n",
    "\n",
    "for l in labels:\n",
    "    right = [data[l].index(d) for d in data[l] if d[0] == l]\n",
    "    wrong = [data[l].index(d) for d in data[l] if d[0] != l]\n",
    "    print(l, right, wrong)\n",
    "    compr_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    compr_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_compr\"]\n",
    "    \n",
    "    suff_ig = [e.score for eva in eva_ig[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    suff_shap = [e.score for eva in eva_shap[l] for e in eva.evaluation_scores if e.name==\"aopc_suff\"]\n",
    "    \n",
    "    compr[l] = {\"IG\": (np.nanmean([e for e in compr_ig if compr_ig.index(e) in right]), np.nanmean([e for e in compr_ig if compr_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in compr_shap if compr_shap.index(e) in right]), np.nanmean([e for e in compr_shap if compr_shap.index(e) in wrong]))}\n",
    "    suff[l] = {\"IG\": (np.nanmean([e for e in suff_ig if suff_ig.index(e) in right]), np.nanmean([e for e in suff_ig if suff_ig.index(e) in wrong])), \n",
    "                \"SHAP\":(np.nanmean([e for e in suff_shap if suff_shap.index(e) in right]), np.nanmean([e for e in suff_shap if suff_shap.index(e) in wrong]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b16b07-ef35-40b7-a4c4-f7b8b1d77cf4",
   "metadata": {},
   "source": [
    "## **2. Sufficiency**<a name=\"suff\"></a>\n",
    "$f(x)_j - f(r_j)_j$ → Score difference once most important tokens included.  \n",
    "**Lower scores better: Inclusion of top 10-100% most important tokens should drive the prediction.**  \n",
    "Measures if top k% (10 step: 10-100) tokens in explanation are sufficient for the right prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7a3eb-930f-4349-96b6-c1dd84d5bd8e",
   "metadata": {},
   "source": [
    "### **2.1. SHAP**<a name=\"suff_shap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a9df8f-9a65-4fcb-b79a-1aef1dd4bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "Zusammenfassung                      [[0.0, 0.01]]\n",
      "Anamnese                             [[0.02, 0.05]]\n",
      "Befunde                              [[0.02, 0.02]]\n",
      "EchoBefunde                          [[0.04, nan]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.06, nan]]\n",
      "Medikation                           [[0.07, nan]]\n",
      "Anrede                               [[0.1, nan]]\n",
      "Abschluss                            [[0.13, 0.35]]\n",
      "Diagnosen                            [[0.19, nan]]\n",
      "KUBefunde                            [[0.26, 0.12]]\n",
      "Mix                                  [[0.4, 0.1]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k == \"SHAP\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9978b-2448-4987-a72a-89e8b683fe17",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726f325e-d511-43cb-b60c-e702f3c18e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: -0.0\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = eva_shap[\"Zusammenfassung\"][1].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = shap[\"Zusammenfassung\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e19545b-ffe1-42d9-91ab-8a8115a700b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.99\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = eva_ig[\"Zusammenfassung\"][1].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = ig[\"Zusammenfassung\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d139da-fc35-43ee-8dfe-f4a45e28c16f",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8898f59f-4e12-4122-afb6-94473d267822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.56\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = eva_shap[\"Mix\"][4].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = shap[\"Mix\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434cefe-5815-4930-bff7-133d00363ecd",
   "metadata": {},
   "source": [
    "### **2.2 Integrated Gradients**<a name=\"suff_ig\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4c3313-86c3-4d9d-a8ba-a0aebe4b4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                Sufficiency mean scores\n",
      "-----------------------------------  -------------------------\n",
      "Befunde                              [[0.34, 0.98]]\n",
      "EchoBefunde                          [[0.37, nan]]\n",
      "Medikation                           [[0.43, nan]]\n",
      "Abschluss                            [[0.43, 0.66]]\n",
      "Zusammenfassung                      [[0.48, 0.01]]\n",
      "Anamnese                             [[0.49, 0.4]]\n",
      "AllergienUnverträglichkeitenRisiken  [[0.6, nan]]\n",
      "Diagnosen                            [[0.66, nan]]\n",
      "Anrede                               [[0.71, nan]]\n",
      "KUBefunde                            [[0.79, 0.61]]\n",
      "Mix                                  [[0.79, 0.95]]\n"
     ]
    }
   ],
   "source": [
    "table = [(l, [[round(s, 2) for s in v] for k, v in suff[l].items() if k==\"IG\"]) for l in labels]\n",
    "table = sorted(table, key = lambda x: x[1][0][0], reverse = False)\n",
    "table.insert(0, [\"Label\", \"Sufficiency mean scores\"])\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f30ef-2192-4742-a7a7-3184fbabded9",
   "metadata": {},
   "source": [
    "#### IG - Best Label: EchoBefunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d4bfd0-dc52-42bf-b532-2d515702d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.34\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = eva_ig[\"EchoBefunde\"][3].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = ig[\"EchoBefunde\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641c64-5e07-4e86-8198-01ae7c16a030",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d709d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.93\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = 0.94 #bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = eva_ig[\"Mix\"][4].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = ig[\"Suff_Mix\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfeb56-d4bc-4df9-a0b5-c23d5cffb429",
   "metadata": {},
   "source": [
    "## Additional Study: Include negative contributing tokens<a name=\"suff_add\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253590d-3900-4358-a248-7abd8a6c7075",
   "metadata": {},
   "source": [
    "#### SHAP - Best Label: Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e354500-6304-4275-a03d-b14d63ccbfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.02\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Zusammenfassung\"][1][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Zusammenfassung\"]\n",
    "metr = eva_shap[\"Zusammenfassung\"][1].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = shap[\"Zusammenfassung\"][\"Add\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ccde5-3147-4e2f-bf87-43e0ccddf424",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens has no significant effect** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54391eb-f7cf-4fe2-89de-a210d9c56b19",
   "metadata": {},
   "source": [
    "#### SHAP - Worst Label: Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74b2849c-dac8-445c-a5df-b3667092e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.65\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = eva_shap[\"Mix\"][4].explanation #bench.explain(sent, target=target)[0] ### SHAP ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = shap[\"Mix\"][\"Add\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a21f52-2efb-4899-aa77-7b777a3024fe",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens worsens score** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dec0de",
   "metadata": {},
   "source": [
    "#### IG - Worst Label: Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5de7268-2570-4b47-8339-cde0537b36f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.66\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"Mix\"][4][1]\n",
    "score = 0.94 #bench.score(sent)\n",
    "target = tag2id[\"Mix\"]\n",
    "metr = eva_ig[\"Mix\"][0].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = ig[\"Suff_Mix\"][\"Add\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd63c9e-7afb-4b71-9f83-04a6c81c6613",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens improves score throughout last half of total steps such that correct label is predicted in last two of them and beforehand scores for false labels are reduced** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fce575-c881-4bce-bf8a-712014ca46a8",
   "metadata": {},
   "source": [
    "#### IG - Best Label: EchoBefunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4180b806-759d-490f-9906-1e96de996eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of all scores: 0.08\n"
     ]
    }
   ],
   "source": [
    "sent = data[\"EchoBefunde\"][3][1]\n",
    "score = 1.0 #bench.score(sent)\n",
    "target = tag2id[\"EchoBefunde\"]\n",
    "metr = eva_ig[\"EchoBefunde\"][3].explanation #bench.explain(sent, target=target)[4] ### IG ###\n",
    "scores = metr.scores[1:-1]\n",
    "tokens = [t if scores[i]>=0 else \"[MASK]\" for i, t in enumerate(metr.tokens[1:-1])] \n",
    "\n",
    "scores = [s for s in scores if s>=0]\n",
    "aggr = []\n",
    "\n",
    "#print(f\"Original sentence: {sent} \\tScore: {round(score[f'LABEL_{target}'],2)}\\nFiltered: {[(t, s) for t, s in zip(tokens, metr.scores[1:-1])]}\\n\")\n",
    "for i in np.arange(.1, 1.1, .1):\n",
    "    sect = round(len(scores)*i)\n",
    "    indices = np.argsort(scores)[::-1][:sect]\n",
    "    filtered = list(filter(lambda x: x!= \"[MASK]\", tokens))\n",
    "    # Get top k tokens\n",
    "    top_tok = [filtered[i] for i in sorted(indices)]\n",
    "    #s = tokenizer.convert_tokens_to_string(top_tok)\n",
    "    new = ig[\"EchoBefunde\"][\"Add\"][i] #score[f\"LABEL_{target}\"] - bench.score(s)[f\"LABEL_{target}\"]\n",
    "    #print(f\"{sect} important token(s) only: '{s}' affects original score: {round(new, 2)} | Labeled: {id2tag[np.argmax(list(bench.score(s).values()))]}: {np.max(list(bench.score(s).values()))}\")\n",
    "    if sect >= 1:\n",
    "        aggr.append(new)\n",
    "\n",
    "print(f\"\\nMean of all scores: {round(mean(set(aggr)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c399a7-74a1-4278-aee7-911e921b8010",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">**! Inclusion of negative tokens in first step improves score substantially such that correct label is predicted** </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
